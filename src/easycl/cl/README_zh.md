[ [English](README.md) | [中文](README_zh.md) ]

# 持续学习方法

本目录包含了多种用于大语言模型的持续学习方法的实现。每种方法都旨在防止在学习新任务时发生灾难性遗忘。

## 已实现的方法

1. **弹性权重固化 (EWC)**
   - 通过基于参数重要性的正则化项来防止灾难性遗忘。
   - 使用Fisher信息矩阵来衡量参数重要性。

2. **无遗忘学习 (LWF)**
   - 使用知识蒸馏来保持先前任务的知识。
   - 无需访问旧任务数据即可保持对先前任务的性能。

3. **经验回放**
   - 通过回放先前任务的子集数据来保持性能。
   - 使用记忆缓冲区来存储和回放过去的经验。

4. **LAMOL (终身语言学习的语言建模)**
   - 使用语言模型生成先前任务的伪样本。
   - 将生成的样本与当前任务数据结合进行训练。

5. **O-LoRA (语言模型持续学习的正交子空间学习)**
   - 通过在不同任务的适应矩阵之间引入正交约束来扩展LoRA。
   - 在保持效率的同时防止灾难性遗忘。

6. **梯度情景记忆 (GEM)**
   - 使用情景记忆投影梯度以防止干扰过去任务的性能。
   - 确保梯度更新保持或提高过去任务的性能。

7. **I-LoRA (基于插值的LoRA)**
   - 构建基于LoRA参数插值的双记忆经验回放框架。
   - 使用EMA进行稳定适配器更新和一致性损失。

8. **MOE-LoRA (专家混合与低秩适应)**
   - 将专家混合架构与LoRA结合以实现高效的模型适应。
   - 使用多个专家LoRA模块进行任务特定专业化。

9. **ABSCL (ABSA LLM-CL)**
   - 使用正交约束训练共享和任务特定适配器。
   - 使用特征统计进行任务特定适配器选择。

10. **Dynamic ConPet**
    - 将共享和任务特定适配器与动态数据集分类相结合。
    - 使用分类器将输入路由到适当的适配器。

11. **CLIT-MoE (任务特定专家混合的持续学习)**
    - 使用双动量专家混合进行持续视觉问答。
    - 实现任务级和实例级路由。

12. **自合成回放 (SSR)**
    - 利用模型的生成能力从先前任务生成伪样本。
    - 使用聚类进行多样化样本选择。

13. **伪回放**
    - SSR方法的简化版本，用于持续学习。
    - 使用基础模型为先前任务生成伪样本。

## 工作流集成

所有方法都通过`tuner.py`文件集成到训练工作流中。每种方法都有其特定的参数，可以通过训练参数进行配置。集成通过以下方式实现：

1. 向训练配置添加方法特定参数
2. 为每种方法实现自定义训练器类
3. 创建工作流文件来处理训练过程
4. 添加必要参数的验证检查

## 原始实现

以下是每个方法的原始实现链接。如果某个方法标记为"NA"，则表示原作者未发布其实现代码。

1. **LAMOL**
   - 原始实现: [待填写链接]
   - 备注: 作者的原始实现

2. **O-LoRA**
   - 原始实现: [待填写链接]
   - 备注: 作者的原始实现

3. **I-LoRA**
   - 原始实现: [待填写链接]
   - 备注: 作者的原始实现

4. **MOE-LoRA**
   - 原始实现: [待填写链接]
   - 备注: 作者的原始实现

5. **ABSCL**
   - 原始实现: [待填写链接]
   - 备注: 作者的原始实现

6. **Dynamic ConPet**
   - 原始实现: [待填写链接]
   - 备注: 作者的原始实现

7. **CLIT-MoE**
   - 原始实现: [待填写链接]
   - 备注: 作者的原始实现

8. **SSR**
   - 原始实现: [待填写链接]
   - 备注: 作者的原始实现 